{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3ae920",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Document datastructure\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35df829d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'CS', 'data_created': '2025-11-01'}, page_content='text content of the documemt')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "  page_content = \"text content of the documemt\",\n",
    "  metadata = {\n",
    "    \"source\":\"example.txt\",\n",
    "    \"pages\":1,\n",
    "    \"author\":\"CS\",\n",
    "    \"data_created\":\"2025-11-01\"\n",
    "  }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a text file\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5bc4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the sample_text dict are created!\n"
     ]
    }
   ],
   "source": [
    "### Creation of two files by code\n",
    "\n",
    "sample_text = {\n",
    "    \"../data/text_files/rag_intro.txt\": \"\"\"RAG stands for Retrieval-Augmented Generation. It combines information retrieval \n",
    "with a language model. The retriever fetches relevant documents from a database or corpus, \n",
    "and the language model generates answers based on the retrieved documents. \n",
    "RAG helps reduce hallucinations in generated content and is widely used in chatbots, \n",
    "Q&A systems, and knowledge assistants.\"\"\",\n",
    "\n",
    "\"../data/text_files/ml_intro.txt\": \"\"\"Machine Learning (ML) is a subset of Artificial Intelligence (AI) \n",
    "that enables systems to learn from data and improve over time without being explicitly programmed. \n",
    "It involves training algorithms on historical data to make predictions or decisions. \n",
    "ML techniques include supervised learning, unsupervised learning, and reinforcement learning. \n",
    "Applications of ML are widespread, including image recognition, natural language processing, \n",
    "recommendation systems, and autonomous vehicles.\"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filePath, fileContent in sample_text.items():\n",
    "  with open(filePath, 'w', encoding='utf-8') as f:\n",
    "    f.write(fileContent)\n",
    "\n",
    "print(\"Files in the sample_text dict are created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf2fd278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/rag_intro.txt'}, page_content='RAG stands for Retrieval-Augmented Generation. It combines information retrieval \\nwith a language model. The retriever fetches relevant documents from a database or corpus, \\nand the language model generates answers based on the retrieved documents. \\nRAG helps reduce hallucinations in generated content and is widely used in chatbots, \\nQ&A systems, and knowledge assistants.')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/rag_intro.txt\", encoding=\"utf-8\")\n",
    "\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea359b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\ml_intro.txt'}, page_content='Machine Learning (ML) is a subset of Artificial Intelligence (AI) \\nthat enables systems to learn from data and improve over time without being explicitly programmed. \\nIt involves training algorithms on historical data to make predictions or decisions. \\nML techniques include supervised learning, unsupervised learning, and reinforcement learning. \\nApplications of ML are widespread, including image recognition, natural language processing, \\nrecommendation systems, and autonomous vehicles.'),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\rag_intro.txt'}, page_content='RAG stands for Retrieval-Augmented Generation. It combines information retrieval \\nwith a language model. The retriever fetches relevant documents from a database or corpus, \\nand the language model generates answers based on the retrieved documents. \\nRAG helps reduce hallucinations in generated content and is widely used in chatbots, \\nQ&A systems, and knowledge assistants.')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Load all the text files from the dircetory\n",
    "dir_loader = DirectoryLoader(\n",
    "  \"../data/text_files\",\n",
    "  glob = \"**/*.txt\", # Pattern to match files\n",
    "  loader_cls = TextLoader,\n",
    "  loader_kwargs={'encoding':'utf-8'},\n",
    "  show_progress=False\n",
    ")\n",
    "\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4036222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-11-24T11:30:40-06:00', 'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'total_pages': 3, 'format': 'PDF 1.7', 'title': '', 'author': 'Chetan Salotra', 'subject': '', 'keywords': '', 'moddate': '2025-11-24T11:30:40-06:00', 'trapped': '', 'modDate': \"D:20251124113040-06'00'\", 'creationDate': \"D:20251124113040-06'00'\", 'page': 0}, page_content='What Are Agentic AI Systems? \\n \\nAgentic AI – The Next Evolution Beyond Chatbots \\n \\nDefinition   \\nAgentic AI refers to autonomous systems that can:   \\n- Understand complex, multi-step goals   \\n- Break goals into tasks   \\n- Use tools (APIs, browsers, code interpreters, databases, etc.)   \\n- Plan, reason, reflect, and self-correct   \\n- Act in loops until the objective is achieved or gracefully fail   \\n \\nUnlike traditional RAG or chat models, agentic systems don’t just answer — they act on the world. \\n \\nCore Characteristics of Agentic AI \\n- Long-running & stateful (memory across turns)   \\n- Tool use & function calling   \\n- Planning & reasoning (Chain-of-Thought, Tree-of-Thought, ReAct)   \\n- Self-reflection & critique   \\n- Multi-agent collaboration (in advanced setups)   \\n \\nPopular Agentic Frameworks (2025) \\n- LangGraph (LangChain) – stateful graphs & cycles   \\n- AutoGen (Microsoft) – multi-agent framework   \\n- CrewAI – role-based agent teams   \\n- OpenAI Swarm – lightweight multi-agent orchestration   \\n- LlamaIndex Workflows & Agents   \\n- AgentGPT / BabyAGI / GodMode (open-source classics)   \\n- MetaGPT – software company simulation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-11-24T11:30:40-06:00', 'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'total_pages': 3, 'format': 'PDF 1.7', 'title': '', 'author': 'Chetan Salotra', 'subject': '', 'keywords': '', 'moddate': '2025-11-24T11:30:40-06:00', 'trapped': '', 'modDate': \"D:20251124113040-06'00'\", 'creationDate': \"D:20251124113040-06'00'\", 'page': 1}, page_content='Real-World Use Cases \\n- Automated software development (SWE-Agent, Devin-style)   \\n- Personal executive assistants (booking, research, emails)   \\n- Autonomous data analysis & reporting   \\n- Web & API automation at scale   \\n- Research agents that read papers, run code, write summaries   \\n \\nSkills & Knowledge You Need to Design & Build Agentic AI \\n \\n1. Core Foundations (Must-Have) \\n- Python proficiency (async, typing, pydantic)   \\n- Deep understanding of modern LLMs (GPT-4o, Claude 3.5/Opus, Grok-2, Llama-3.1/3.2 405B, \\nDeepSeek-R1)   \\n- Prompt engineering + Chain-of-Thought, ReAct, Reflexion techniques   \\n- Function calling / tool use (OpenAI, Anthropic, Gemini, Groq, Fireworks schemas)   \\n \\n2. Agent Architecture & Orchestration \\n- LangChain / LlamaIndex (LCEL, expressions, agents)   \\n- LangGraph (cycles, state machines, persistence, human-in-the-loop)   \\n- CrewAI or AutoGen for multi-agent systems   \\n- Memory types: short-term, long-term, vector memory, entity memory   \\n \\n3. Tooling & Integrations \\n- Building custom tools (FastAPI, Pydantic, JSON mode)   \\n- Browser automation (Playwright, Selenium, Browserbase)   \\n- Code execution sandboxes (E2B, Docker, Jupyter kernels)   \\n- Search tools (Tavily, Exa, SerpAPI, Perplexity API)   \\n- Database & vector store integrations   \\n \\n4. Planning & Reasoning Techniques'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-11-24T11:30:40-06:00', 'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'total_pages': 3, 'format': 'PDF 1.7', 'title': '', 'author': 'Chetan Salotra', 'subject': '', 'keywords': '', 'moddate': '2025-11-24T11:30:40-06:00', 'trapped': '', 'modDate': \"D:20251124113040-06'00'\", 'creationDate': \"D:20251124113040-06'00'\", 'page': 2}, page_content='- ReAct (Reason + Act)   \\n- Plan-and-Execute   \\n- Tree-of-Thoughts (ToT)   \\n- Graph-based planning   \\n- Self-reflection & critique loops   \\n \\n5. Evaluation & Guardrails \\n- Ragas, DeepEval, TruLens for agent trajectories   \\n- Logging & tracing (LangSmith, Phoenix, Helicone)   \\n- Safety: output validation, moderation, sandboxing   \\n \\n6. Production & Deployment \\n- Async workflows (FastAPI + Celery/Redis)   \\n- Streaming & real-time updates   \\n- Cost control & token management   \\n- Scaling (Kubernetes, serverless, Modal, RunPod)   \\n \\nLearning Path Summary (3–6 months to become job-ready) \\nMonth 1–2 → Master LangChain/LangGraph + build simple ReAct agents   \\nMonth 3 → Multi-tool agents + memory + LangGraph cycles   \\nMonth 4 → Multi-agent systems (CrewAI/AutoGen) + evaluation   \\nMonth 5–6 → Build a full end-to-end agent (e.g., autonomous research analyst or personal assistant) and \\ndeploy it   \\n \\nAgentic AI is currently the fastest-growing segment in applied AI. Companies are aggressively hiring \\nengineers who can ship reliable, tool-using, autonomous agents.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-11-24T11:24:11-06:00', 'source': '..\\\\data\\\\pdf_files\\\\RAG_Overview.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RAG_Overview.pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Chetan Salotra', 'subject': '', 'keywords': '', 'moddate': '2025-11-24T11:24:11-06:00', 'trapped': '', 'modDate': \"D:20251124112411-06'00'\", 'creationDate': \"D:20251124112411-06'00'\", 'page': 0}, page_content='Retrieval-Augmented Generation (RAG) \\n \\nWhat is RAG? \\nRetrieval-Augmented Generation (RAG) is an architecture that combines a retriever and a generative \\nlarge language model (LLM). Instead of answering solely from the model’s trained parameters, RAG \\nfetches up-to-date or domain-specific documents in real time and uses them as additional context for the \\nLLM. \\n \\nHow RAG Works (4 steps) \\n1. User asks a question   \\n2. Retriever (dense vector search) finds the top-k most relevant text chunks from an external knowledge \\nbase (using embeddings)   \\n3. Retrieved chunks are injected into the LLM prompt as context   \\n4. LLM generates a grounded, accurate answer and (optionally) cites the sources   \\n \\nKey Advantages \\n- Dramatically reduces hallucinations   \\n- Works with private, proprietary, or constantly changing data   \\n- No need to retrain or fine-tune the LLM — just update the vector database   \\n- Cost-efficient and scalable   \\n- Provides traceability and citations   \\n \\nCommon Use Cases \\n- Enterprise search & chatbots   \\n- Legal, medical, and financial Q&A   \\n- Customer support with internal docs   \\n- Research assistants over latest papers or reports   \\n- Personal assistants using your own files   \\n \\nPopular Tools & Stack (2025) \\n- Frameworks: LangChain, LlamaIndex, Haystack, Flowise   \\n- Vector databases: Pinecone, Weaviate, Chroma, Qdrant, Milvus, PGVector   \\n- Embedding models: text-embedding-3-large (OpenAI), voyage-law-2, BGE-large, Cohere embed-v3   \\n- Evaluation: Ragas, DeepEval, TruLens   \\n \\nRAG has become the default method for building reliable, knowledge-intensive AI applications that stay \\naccurate and current without constant model retraining.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### PDF Loader, process PDFs\n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "  \"../data/pdf_files\",\n",
    "  glob = \"**/*.pdf\", # Pattern to match files\n",
    "  loader_cls = PyMuPDFLoader,\n",
    "  show_progress=False\n",
    ")\n",
    "\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49167c5",
   "metadata": {},
   "source": [
    "## RAG Pipelines - Data Ingestion to Vector DB pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8ad4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a21b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 2 files found in the directory\n",
      "\n",
      "Processing: Agentic_AI_Roadmap.pdf\n",
      "Loaded 3 pages\n",
      "\n",
      "Processing: RAG_Overview.pdf\n",
      "Loaded 1 pages\n",
      "\n",
      "total documents loaded: 4\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdfs inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "  \"\"\"Process all PDF files in a directory\"\"\"\n",
    "  doc_list = []\n",
    "  directory_path = Path(pdf_directory)\n",
    "\n",
    "  # Find all pdf files recursively\n",
    "  pdf_list = list(directory_path.rglob(\"**/*.pdf\"))\n",
    "\n",
    "  print(f\"Total {len(pdf_list)} files found in the directory\")\n",
    "\n",
    "  for pdf_file in pdf_list:\n",
    "    print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "    try:\n",
    "      loader = PyMuPDFLoader(pdf_file)\n",
    "      documents = loader.load()\n",
    "\n",
    "      # Add source information in the document metadata\n",
    "      for doc in documents:\n",
    "        doc.metadata['source_file'] = pdf_file.name\n",
    "        doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "      doc_list.extend(documents)\n",
    "      print(f\"Loaded {len(documents)} pages\")\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Error occured while processing pdf{e}\")\n",
    "\n",
    "  print(f\"\\ntotal documents loaded: {len(doc_list)}\")\n",
    "  return doc_list\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75320c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-11-24T11:30:40-06:00', 'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'total_pages': 3, 'format': 'PDF 1.7', 'title': '', 'author': 'Chetan Salotra', 'subject': '', 'keywords': '', 'moddate': '2025-11-24T11:30:40-06:00', 'trapped': '', 'modDate': \"D:20251124113040-06'00'\", 'creationDate': \"D:20251124113040-06'00'\", 'page': 0, 'source_file': 'Agentic_AI_Roadmap.pdf', 'file_type': 'pdf'}, page_content='What Are Agentic AI Systems? \\n \\nAgentic AI – The Next Evolution Beyond Chatbots \\n \\nDefinition   \\nAgentic AI refers to autonomous systems that can:   \\n- Understand complex, multi-step goals   \\n- Break goals into tasks   \\n- Use tools (APIs, browsers, code interpreters, databases, etc.)   \\n- Plan, reason, reflect, and self-correct   \\n- Act in loops until the objective is achieved or gracefully fail   \\n \\nUnlike traditional RAG or chat models, agentic systems don’t just answer — they act on the world. \\n \\nCore Characteristics of Agentic AI \\n- Long-running & stateful (memory across turns)   \\n- Tool use & function calling   \\n- Planning & reasoning (Chain-of-Thought, Tree-of-Thought, ReAct)   \\n- Self-reflection & critique   \\n- Multi-agent collaboration (in advanced setups)   \\n \\nPopular Agentic Frameworks (2025) \\n- LangGraph (LangChain) – stateful graphs & cycles   \\n- AutoGen (Microsoft) – multi-agent framework   \\n- CrewAI – role-based agent teams   \\n- OpenAI Swarm – lightweight multi-agent orchestration   \\n- LlamaIndex Workflows & Agents   \\n- AgentGPT / BabyAGI / GodMode (open-source classics)   \\n- MetaGPT – software company simulation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-11-24T11:30:40-06:00', 'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'total_pages': 3, 'format': 'PDF 1.7', 'title': '', 'author': 'Chetan Salotra', 'subject': '', 'keywords': '', 'moddate': '2025-11-24T11:30:40-06:00', 'trapped': '', 'modDate': \"D:20251124113040-06'00'\", 'creationDate': \"D:20251124113040-06'00'\", 'page': 1, 'source_file': 'Agentic_AI_Roadmap.pdf', 'file_type': 'pdf'}, page_content='Real-World Use Cases \\n- Automated software development (SWE-Agent, Devin-style)   \\n- Personal executive assistants (booking, research, emails)   \\n- Autonomous data analysis & reporting   \\n- Web & API automation at scale   \\n- Research agents that read papers, run code, write summaries   \\n \\nSkills & Knowledge You Need to Design & Build Agentic AI \\n \\n1. Core Foundations (Must-Have) \\n- Python proficiency (async, typing, pydantic)   \\n- Deep understanding of modern LLMs (GPT-4o, Claude 3.5/Opus, Grok-2, Llama-3.1/3.2 405B, \\nDeepSeek-R1)   \\n- Prompt engineering + Chain-of-Thought, ReAct, Reflexion techniques   \\n- Function calling / tool use (OpenAI, Anthropic, Gemini, Groq, Fireworks schemas)   \\n \\n2. Agent Architecture & Orchestration \\n- LangChain / LlamaIndex (LCEL, expressions, agents)   \\n- LangGraph (cycles, state machines, persistence, human-in-the-loop)   \\n- CrewAI or AutoGen for multi-agent systems   \\n- Memory types: short-term, long-term, vector memory, entity memory   \\n \\n3. Tooling & Integrations \\n- Building custom tools (FastAPI, Pydantic, JSON mode)   \\n- Browser automation (Playwright, Selenium, Browserbase)   \\n- Code execution sandboxes (E2B, Docker, Jupyter kernels)   \\n- Search tools (Tavily, Exa, SerpAPI, Perplexity API)   \\n- Database & vector store integrations   \\n \\n4. Planning & Reasoning Techniques'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-11-24T11:30:40-06:00', 'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'total_pages': 3, 'format': 'PDF 1.7', 'title': '', 'author': 'Chetan Salotra', 'subject': '', 'keywords': '', 'moddate': '2025-11-24T11:30:40-06:00', 'trapped': '', 'modDate': \"D:20251124113040-06'00'\", 'creationDate': \"D:20251124113040-06'00'\", 'page': 2, 'source_file': 'Agentic_AI_Roadmap.pdf', 'file_type': 'pdf'}, page_content='- ReAct (Reason + Act)   \\n- Plan-and-Execute   \\n- Tree-of-Thoughts (ToT)   \\n- Graph-based planning   \\n- Self-reflection & critique loops   \\n \\n5. Evaluation & Guardrails \\n- Ragas, DeepEval, TruLens for agent trajectories   \\n- Logging & tracing (LangSmith, Phoenix, Helicone)   \\n- Safety: output validation, moderation, sandboxing   \\n \\n6. Production & Deployment \\n- Async workflows (FastAPI + Celery/Redis)   \\n- Streaming & real-time updates   \\n- Cost control & token management   \\n- Scaling (Kubernetes, serverless, Modal, RunPod)   \\n \\nLearning Path Summary (3–6 months to become job-ready) \\nMonth 1–2 → Master LangChain/LangGraph + build simple ReAct agents   \\nMonth 3 → Multi-tool agents + memory + LangGraph cycles   \\nMonth 4 → Multi-agent systems (CrewAI/AutoGen) + evaluation   \\nMonth 5–6 → Build a full end-to-end agent (e.g., autonomous research analyst or personal assistant) and \\ndeploy it   \\n \\nAgentic AI is currently the fastest-growing segment in applied AI. Companies are aggressively hiring \\nengineers who can ship reliable, tool-using, autonomous agents.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-11-24T11:24:11-06:00', 'source': '..\\\\data\\\\pdf_files\\\\RAG_Overview.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RAG_Overview.pdf', 'total_pages': 1, 'format': 'PDF 1.7', 'title': '', 'author': 'Chetan Salotra', 'subject': '', 'keywords': '', 'moddate': '2025-11-24T11:24:11-06:00', 'trapped': '', 'modDate': \"D:20251124112411-06'00'\", 'creationDate': \"D:20251124112411-06'00'\", 'page': 0, 'source_file': 'RAG_Overview.pdf', 'file_type': 'pdf'}, page_content='Retrieval-Augmented Generation (RAG) \\n \\nWhat is RAG? \\nRetrieval-Augmented Generation (RAG) is an architecture that combines a retriever and a generative \\nlarge language model (LLM). Instead of answering solely from the model’s trained parameters, RAG \\nfetches up-to-date or domain-specific documents in real time and uses them as additional context for the \\nLLM. \\n \\nHow RAG Works (4 steps) \\n1. User asks a question   \\n2. Retriever (dense vector search) finds the top-k most relevant text chunks from an external knowledge \\nbase (using embeddings)   \\n3. Retrieved chunks are injected into the LLM prompt as context   \\n4. LLM generates a grounded, accurate answer and (optionally) cites the sources   \\n \\nKey Advantages \\n- Dramatically reduces hallucinations   \\n- Works with private, proprietary, or constantly changing data   \\n- No need to retrain or fine-tune the LLM — just update the vector database   \\n- Cost-efficient and scalable   \\n- Provides traceability and citations   \\n \\nCommon Use Cases \\n- Enterprise search & chatbots   \\n- Legal, medical, and financial Q&A   \\n- Customer support with internal docs   \\n- Research assistants over latest papers or reports   \\n- Personal assistants using your own files   \\n \\nPopular Tools & Stack (2025) \\n- Frameworks: LangChain, LlamaIndex, Haystack, Flowise   \\n- Vector databases: Pinecone, Weaviate, Chroma, Qdrant, Milvus, PGVector   \\n- Embedding models: text-embedding-3-large (OpenAI), voyage-law-2, BGE-large, Cohere embed-v3   \\n- Evaluation: Ragas, DeepEval, TruLens   \\n \\nRAG has become the default method for building reliable, knowledge-intensive AI applications that stay \\naccurate and current without constant model retraining.')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c5985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting into chunks\n",
    "def split_documents(documents, chunk_size=600, chunk_overlap=60):\n",
    "  \"\"\"Split documents into smaller chunks so the embedding model can represent them accurately\"\"\"\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "  )\n",
    "\n",
    "  splitted_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "  print(f\"split {len(documents)} documents into {len(splitted_docs)} chunks\")\n",
    "\n",
    "  # show example of a chunk\n",
    "  if splitted_docs:\n",
    "    print(f\"\\nExample chunk:\")\n",
    "    print(f\"Content: {splitted_docs[0].page_content[:200]}...\")\n",
    "    print(f\"Metadata: {splitted_docs[0].metadata}\")\n",
    "\n",
    "  return splitted_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f1242d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 4 documents into 12 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: What Are Agentic AI Systems? \n",
      " \n",
      "Agentic AI – The Next Evolution Beyond Chatbots \n",
      " \n",
      "Definition   \n",
      "Agentic AI refers to autonomous systems that can:   \n",
      "- Understand complex, multi-step goals   \n",
      "- Break ...\n",
      "Metadata: {'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-11-24T11:30:40-06:00', 'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf', 'total_pages': 3, 'format': 'PDF 1.7', 'title': '', 'author': 'Chetan Salotra', 'subject': '', 'keywords': '', 'moddate': '2025-11-24T11:30:40-06:00', 'trapped': '', 'modDate': \"D:20251124113040-06'00'\", 'creationDate': \"D:20251124113040-06'00'\", 'page': 0, 'source_file': 'Agentic_AI_Roadmap.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a7215",
   "metadata": {},
   "source": [
    "### Embeddings And VectorStoreDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36367e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "281a6ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension : 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Embedder at 0x2313a9bcb90>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedder:\n",
    "  \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "\n",
    "  def __init__(self, model_name:str=\"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Initialize the embedder class\n",
    "\n",
    "    Args:\n",
    "        model_name: HuggingFace model name for sentence embeddings\n",
    "\n",
    "    \"\"\"\n",
    "    self.model_name = model_name\n",
    "    self.model = None\n",
    "    self._load_model()\n",
    "\n",
    "\n",
    "  def _load_model(self):\n",
    "    \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "    try:\n",
    "      print(f\"Loading the embedding model: {self.model_name}\")\n",
    "      self.model = SentenceTransformer(self.model_name)\n",
    "      print(f\"Model loaded successfully. Embedding dimension : {self.model.get_sentence_embedding_dimension()}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error loading model {self.model_name} : {e}\")\n",
    "      raise ValueError(\"Model not loaded\")\n",
    "\n",
    "\n",
    "  def generate_embeddings(self, texts:List[str])-> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts\n",
    "\n",
    "    Args:\n",
    "      texts: List of text strings to embed\n",
    "\n",
    "    Returns:\n",
    "      numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "    \"\"\"\n",
    "    if not self.model:\n",
    "      raise ValueError(\"Model not loaded\")\n",
    "\n",
    "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "    embeddings = self.model.encode(texts, show_progress_bar = False)\n",
    "    print(f\"generating embeddings with shape: {embeddings.shape}\")\n",
    "    return embeddings\n",
    "  \n",
    "\n",
    "  def get_embedding_dimension(self)-> int:\n",
    "    \"\"\"Getting embedding dimension of the model\"\"\"\n",
    "    if not self.model:\n",
    "      raise ValueError(\"Model not loaded\")\n",
    "    return self.model.get_sentence_embedding_dimension()\n",
    "  \n",
    "\n",
    "#Initialize the Embedder\n",
    "embedder = Embedder()\n",
    "embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38176f92",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc54b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in the collection: 55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2313a9c9a90>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "  \"\"\"Manages document embeddings in a chromaDB vector store\"\"\"\n",
    "\n",
    "  def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "    \"\"\"\n",
    "    Initialize the vector store\n",
    "\n",
    "    Args:\n",
    "      collection_name: name of the ChromaDB collection\n",
    "      persist_directory: Directory to persist the vector store \n",
    "    \"\"\"\n",
    "    self.collection_name = collection_name\n",
    "    self.persist_directory = persist_directory\n",
    "    self.client = None\n",
    "    self.collection = None\n",
    "    self._initialize_store()\n",
    "\n",
    "\n",
    "  def _initialize_store(self):\n",
    "    \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "    try:\n",
    "      os.makedirs(self.persist_directory, exist_ok= True)\n",
    "      self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "      # Get or create collection\n",
    "      self.collection = self.client.get_or_create_collection(\n",
    "        name=self.collection_name,\n",
    "        metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "      )\n",
    "      print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "      print(f\"Existing documents in the collection: {self.collection.count()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Error initializing the vector store: {e}\")\n",
    "      raise ValueError(\"Vector store can be initialized\")\n",
    "      \n",
    "\n",
    "  def add_documents(self, documents:List[Any], embeddings):\n",
    "    \"\"\"\n",
    "    Add documents and their embeddings to the vector store\n",
    "\n",
    "    Args:\n",
    "        documents: List of LangChain documents\n",
    "        embeddings: Corresponding embeddings for the documents\n",
    "    \"\"\"  \n",
    "\n",
    "    if len(documents) != len(embeddings):\n",
    "      raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "    \n",
    "    print(f\" Adding {len(documents)} to the Vector store\")\n",
    "\n",
    "    # Prepare storage lists/data for ChromaDB \n",
    "    ids = []\n",
    "    texts = []\n",
    "    metas = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "      # Generate unique ID\n",
    "      doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "      ids.append(doc_id)\n",
    "\n",
    "      # Prepare metadata\n",
    "      metadata = dict(doc.metadata)\n",
    "      metadata['doc_index'] = i\n",
    "      metadata['content_length'] = len(doc.page_content)\n",
    "      metas.append(metadata)\n",
    "\n",
    "      # Document content\n",
    "      texts.append(doc.page_content)\n",
    "\n",
    "      # Embedding\n",
    "      embeddings_list.append(embedding.tolist())\n",
    "\n",
    "    # Add to collection\n",
    "    try:\n",
    "      self.collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=embeddings_list,\n",
    "        metadatas=metas,\n",
    "        documents=texts\n",
    "      )\n",
    "      print(f\"Successfully added {len(documents)} documents\")\n",
    "      print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Error adding documents to vector store: {e}\")\n",
    "      raise\n",
    "\n",
    "VectorStore = VectorStore()\n",
    "VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc67617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 12 texts...\n",
      "generating embeddings with shape: (12, 384)\n",
      " Adding 12 to the Vector store\n",
      "Successfully added 12 documents\n",
      "Total documents in collection: 67\n"
     ]
    }
   ],
   "source": [
    "#Covert text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "#Generate the Embeddings\n",
    "embeddings = embedder.generate_embeddings(texts)\n",
    "\n",
    "#store in the vector database\n",
    "VectorStore.add_documents(chunks, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d2220",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b27520cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x2313a485d50>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAGRetriever:\n",
    "  \"\"\" Handles query-based retriever from the vector store\"\"\"\n",
    "\n",
    "  def __init__(self, vector_store: VectorStore, embedder: Embedder):\n",
    "    \"\"\"\n",
    "    Initialize the retriever\n",
    "\n",
    "    Args:\n",
    "        vector_store: Vector store containing document embeddings\n",
    "        embedding_manager: Manager for generating query embeddings\n",
    "    \"\"\"\n",
    "    self.vector_store = vector_store\n",
    "    self.embedder = embedder\n",
    "\n",
    "\n",
    "  def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents based on a query.\n",
    "\n",
    "    Args:\n",
    "        query: Text query string.\n",
    "        top_k: Number of top results to return.\n",
    "        score_threshold: Minimum similarity score to include a chunk.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts -> [{ \"text\": ..., \"metadata\": ..., \"score\": ... }]\n",
    "    \"\"\"\n",
    "    print(f\"Retrieving documents for query: '{query}'\")\n",
    "    print(f\"Top K: {top_k}, score threshold: {score_threshold}\")\n",
    "\n",
    "    #Generate embedding for the query\n",
    "    query_embedding = self.embedder.generate_embeddings([query])[0]\n",
    "\n",
    "    #Fetch from the vector store\n",
    "    try:\n",
    "        results = self.vector_store.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=top_k\n",
    "        )\n",
    "\n",
    "        #Process results\n",
    "        retrieved_docs = []\n",
    "\n",
    "        if results['documents'] and results['documents'][0]:\n",
    "            documents = results['documents'][0]\n",
    "            metadatas = results['metadatas'][0]\n",
    "            distances = results['distances'][0]\n",
    "            ids = results['ids'][0]\n",
    "\n",
    "            for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "               #Convert distance to similarity score (chromadb uses cosine distance)\n",
    "                similarity_score = (2 - distance)/2  \n",
    "\n",
    "                print(\"similarity\", similarity_score)\n",
    "                if similarity_score >= score_threshold:\n",
    "                    retrieved_docs.append({\n",
    "                        'id':doc_id,\n",
    "                        'content':document,\n",
    "                        'metadata':metadata,\n",
    "                        'similarity_score':similarity_score,\n",
    "                        'distance':distance,\n",
    "                        'rank':i+1\n",
    "                    })\n",
    "            print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")    \n",
    "        else:\n",
    "            print(\"No documents found\")\n",
    "\n",
    "        return retrieved_docs\n",
    "    \n",
    "    except Exception as e:\n",
    "       print(f\"Error during retrieval: {e}\")\n",
    "       return []\n",
    "    \n",
    "\n",
    "rag_retriever = RAGRetriever(VectorStore, embedder)\n",
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "03172da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Agents created '\n",
      "Top K: 5, score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n",
      "generating embeddings with shape: (1, 384)\n",
      "similarity 0.5494636297225952\n",
      "similarity 0.5373220443725586\n",
      "similarity 0.5373220443725586\n",
      "similarity 0.5311039984226227\n",
      "similarity 0.5311039984226227\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_ac0c039f_1',\n",
       "  'content': 'Unlike traditional RAG or chat models, agentic systems don’t just answer — they act on the world. \\n \\nCore Characteristics of Agentic AI \\n- Long-running & stateful (memory across turns)   \\n- Tool use & function calling   \\n- Planning & reasoning (Chain-of-Thought, Tree-of-Thought, ReAct)   \\n- Self-reflection & critique   \\n- Multi-agent collaboration (in advanced setups)   \\n \\nPopular Agentic Frameworks (2025) \\n- LangGraph (LangChain) – stateful graphs & cycles',\n",
       "  'metadata': {'creationdate': '2025-11-24T11:30:40-06:00',\n",
       "   'content_length': 461,\n",
       "   'trapped': '',\n",
       "   'creationDate': \"D:20251124113040-06'00'\",\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 3,\n",
       "   'creator': 'Microsoft® Word for Microsoft 365',\n",
       "   'keywords': '',\n",
       "   'moddate': '2025-11-24T11:30:40-06:00',\n",
       "   'format': 'PDF 1.7',\n",
       "   'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf',\n",
       "   'author': 'Chetan Salotra',\n",
       "   'doc_index': 1,\n",
       "   'source_file': 'Agentic_AI_Roadmap.pdf',\n",
       "   'producer': 'Microsoft® Word for Microsoft 365',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf',\n",
       "   'page': 0,\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'modDate': \"D:20251124113040-06'00'\"},\n",
       "  'similarity_score': 0.5494636297225952,\n",
       "  'distance': 0.9010727405548096,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_1be8748a_3',\n",
       "  'content': 'Popular Agentic Frameworks (2025) \\n- LangGraph (LangChain) – stateful graphs & cycles   \\n- AutoGen (Microsoft) – multi-agent framework   \\n- CrewAI – role-based agent teams   \\n- OpenAI Swarm – lightweight multi-agent orchestration   \\n- LlamaIndex Workflows & Agents',\n",
       "  'metadata': {'total_pages': 3,\n",
       "   'moddate': '2025-11-24T11:30:40-06:00',\n",
       "   'title': '',\n",
       "   'format': 'PDF 1.7',\n",
       "   'modDate': \"D:20251124113040-06'00'\",\n",
       "   'producer': 'Microsoft® Word for Microsoft 365',\n",
       "   'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf',\n",
       "   'author': 'Chetan Salotra',\n",
       "   'doc_index': 3,\n",
       "   'content_length': 264,\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf',\n",
       "   'creationDate': \"D:20251124113040-06'00'\",\n",
       "   'file_type': 'pdf',\n",
       "   'subject': '',\n",
       "   'trapped': '',\n",
       "   'creator': 'Microsoft® Word for Microsoft 365',\n",
       "   'page': 0,\n",
       "   'creationdate': '2025-11-24T11:30:40-06:00',\n",
       "   'keywords': '',\n",
       "   'source_file': 'Agentic_AI_Roadmap.pdf'},\n",
       "  'similarity_score': 0.5373220443725586,\n",
       "  'distance': 0.9253559112548828,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_bcf9ebec_3',\n",
       "  'content': 'Popular Agentic Frameworks (2025) \\n- LangGraph (LangChain) – stateful graphs & cycles   \\n- AutoGen (Microsoft) – multi-agent framework   \\n- CrewAI – role-based agent teams   \\n- OpenAI Swarm – lightweight multi-agent orchestration   \\n- LlamaIndex Workflows & Agents',\n",
       "  'metadata': {'moddate': '2025-11-24T11:30:40-06:00',\n",
       "   'title': '',\n",
       "   'creator': 'Microsoft® Word for Microsoft 365',\n",
       "   'source_file': 'Agentic_AI_Roadmap.pdf',\n",
       "   'modDate': \"D:20251124113040-06'00'\",\n",
       "   'total_pages': 3,\n",
       "   'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf',\n",
       "   'producer': 'Microsoft® Word for Microsoft 365',\n",
       "   'creationdate': '2025-11-24T11:30:40-06:00',\n",
       "   'subject': '',\n",
       "   'page': 0,\n",
       "   'author': 'Chetan Salotra',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf',\n",
       "   'format': 'PDF 1.7',\n",
       "   'creationDate': \"D:20251124113040-06'00'\",\n",
       "   'file_type': 'pdf',\n",
       "   'keywords': '',\n",
       "   'content_length': 264,\n",
       "   'doc_index': 3,\n",
       "   'trapped': ''},\n",
       "  'similarity_score': 0.5373220443725586,\n",
       "  'distance': 0.9253559112548828,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_8ee42916_1',\n",
       "  'content': '- Plan, reason, reflect, and self-correct   \\n- Act in loops until the objective is achieved or gracefully fail   \\n \\nUnlike traditional RAG or chat models, agentic systems don’t just answer — they act on the world. \\n \\nCore Characteristics of Agentic AI',\n",
       "  'metadata': {'total_pages': 3,\n",
       "   'creator': 'Microsoft® Word for Microsoft 365',\n",
       "   'trapped': '',\n",
       "   'source_file': 'Agentic_AI_Roadmap.pdf',\n",
       "   'content_length': 251,\n",
       "   'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf',\n",
       "   'format': 'PDF 1.7',\n",
       "   'producer': 'Microsoft® Word for Microsoft 365',\n",
       "   'page': 0,\n",
       "   'author': 'Chetan Salotra',\n",
       "   'title': '',\n",
       "   'keywords': '',\n",
       "   'subject': '',\n",
       "   'moddate': '2025-11-24T11:30:40-06:00',\n",
       "   'file_type': 'pdf',\n",
       "   'modDate': \"D:20251124113040-06'00'\",\n",
       "   'doc_index': 1,\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf',\n",
       "   'creationdate': '2025-11-24T11:30:40-06:00',\n",
       "   'creationDate': \"D:20251124113040-06'00'\"},\n",
       "  'similarity_score': 0.5311039984226227,\n",
       "  'distance': 0.9377920031547546,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_43c5ca97_1',\n",
       "  'content': '- Plan, reason, reflect, and self-correct   \\n- Act in loops until the objective is achieved or gracefully fail   \\n \\nUnlike traditional RAG or chat models, agentic systems don’t just answer — they act on the world. \\n \\nCore Characteristics of Agentic AI',\n",
       "  'metadata': {'trapped': '',\n",
       "   'producer': 'Microsoft® Word for Microsoft 365',\n",
       "   'keywords': '',\n",
       "   'creator': 'Microsoft® Word for Microsoft 365',\n",
       "   'file_path': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf',\n",
       "   'author': 'Chetan Salotra',\n",
       "   'content_length': 251,\n",
       "   'page': 0,\n",
       "   'format': 'PDF 1.7',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'source_file': 'Agentic_AI_Roadmap.pdf',\n",
       "   'moddate': '2025-11-24T11:30:40-06:00',\n",
       "   'file_type': 'pdf',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\Agentic_AI_Roadmap.pdf',\n",
       "   'creationdate': '2025-11-24T11:30:40-06:00',\n",
       "   'creationDate': \"D:20251124113040-06'00'\",\n",
       "   'total_pages': 3,\n",
       "   'doc_index': 1,\n",
       "   'modDate': \"D:20251124113040-06'00'\"},\n",
       "  'similarity_score': 0.5311039984226227,\n",
       "  'distance': 0.9377920031547546,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Agents created \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9537fa",
   "metadata": {},
   "source": [
    "### Integration Vectordb Context pipeline with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=api_key  \n",
    ")\n",
    "\n",
    "##Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "  ## retrieve the context\n",
    "  results = retriever.retrieve(query, top_k = top_k)\n",
    "  context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "\n",
    "  if not context:\n",
    "    return \"No relevant context found to answer the question\"\n",
    "  \n",
    "  ##generate the answer using GROQ LLM\n",
    "  prompt=f\"\"\"Use the following context to answer the question conscisely.\n",
    "      Context: {context}\n",
    "\n",
    "      Question: {query}\n",
    "\n",
    "      Answer:\"\"\"\n",
    "  \n",
    "  response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "  return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "59da1c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is Agentic AI?'\n",
      "Top K: 3, score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n",
      "generating embeddings with shape: (1, 384)\n",
      "similarity 0.7794758677482605\n",
      "similarity 0.7765256464481354\n",
      "similarity 0.7703945934772491\n",
      "Retrieved 3 documents (after filtering)\n",
      "Agentic AI refers to autonomous systems that can understand complex, multi-step goals, break them into tasks, and use tools (APIs, browsers, code interpreters, etc.). They plan, reason, reflect, self-correct, and act in loops until an objective is achieved or gracefully fails, distinguishing them from traditional RAG or chat models by acting on the world.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"what is Agentic AI?\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b7506",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a39870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Give complete list of the core skills required t o build agentic AI?'\n",
      "Top K: 5, score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n",
      "generating embeddings with shape: (1, 384)\n",
      "similarity 0.7320262491703033\n",
      "similarity 0.7320262491703033\n",
      "similarity 0.704714298248291\n",
      "similarity 0.704714298248291\n",
      "similarity 0.6919834911823273\n",
      "Retrieved 5 documents (after filtering)\n",
      "Answer: The core skills required to build agentic AI are:\n",
      "*   Python proficiency (async, typing, pydantic)\n",
      "*   Deep understanding of modern LLMs (GPT-4o, Claude 3.5/Opus, Grok-2, Llama-3.1/3.2 405B, DeepSeek-R1)\n",
      "Sources: [{'source': 'Agentic_AI_Roadmap.pdf', 'page': 1, 'score': 0.7320262491703033, 'preview': 'Skills & Knowledge You Need to Design & Build Agentic AI \\n \\n1. Core Foundations (Must-Have) \\n- Python proficiency (async...'}, {'source': 'Agentic_AI_Roadmap.pdf', 'page': 1, 'score': 0.7320262491703033, 'preview': 'Skills & Knowledge You Need to Design & Build Agentic AI \\n \\n1. Core Foundations (Must-Have) \\n- Python proficiency (async...'}, {'source': 'Agentic_AI_Roadmap.pdf', 'page': 0, 'score': 0.704714298248291, 'preview': 'Core Characteristics of Agentic AI \\n- Long-running & stateful (memory across turns)   \\n- Tool use & function calling   \\n...'}, {'source': 'Agentic_AI_Roadmap.pdf', 'page': 0, 'score': 0.704714298248291, 'preview': 'Core Characteristics of Agentic AI \\n- Long-running & stateful (memory across turns)   \\n- Tool use & function calling   \\n...'}, {'source': 'Agentic_AI_Roadmap.pdf', 'page': 1, 'score': 0.6919834911823273, 'preview': 'Real-World Use Cases \\n- Automated software development (SWE-Agent, Devin-style)   \\n- Personal executive assistants (book...'}]\n",
      "Confidence: 0.7320262491703033\n",
      "Context Preview: Skills & Knowledge You Need to Design & Build Agentic AI \n",
      " \n",
      "1. Core Foundations (Must-Have) \n",
      "- Python proficiency (async, typing, pydantic)   \n",
      "- Deep understanding of modern LLMs (GPT-4o, Claude 3.5/Opus, Grok-2, Llama-3.1/3.2 405B, \n",
      "DeepSeek-R1)\n",
      "\n",
      "Skills & Knowledge You Need to Design & Build Agenti\n"
     ]
    }
   ],
   "source": [
    "### Enhanced RAG Pipeline Features\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "  \"\"\"\n",
    "  RAG pipeline with extra features:\n",
    "  - Returns answer, sources, confidence score, and optionally full context.\n",
    "  \"\"\"\n",
    "  results = retriever.retrieve(query, top_k=top_k, score_threshold =min_score)\n",
    "  if not results:\n",
    "    return {'answer': 'No relevenat context found.', 'sources':[], 'confidence':0.0, 'context':''}\n",
    "  \n",
    "  #Prepare context and sources\n",
    "  context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "  sources = [{\n",
    "    'source':doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "    'page':doc['metadata'].get('page', 'unknown'),\n",
    "    'score':doc['similarity_score'],\n",
    "    'preview':doc['content'][:120] + '...'\n",
    "  } for doc in results]\n",
    "  confidence = max([doc['similarity_score'] for doc in results])\n",
    "\n",
    "  #Generate answer\n",
    "  prompt = f\"\"\"Use the following context to answer the question concisely. \\n{context}\\n\\nQuestion: {query} \\n\\nAnswer:\"\"\"\n",
    "  response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "\n",
    "  output = {\n",
    "    'answer':response.content,\n",
    "    'sources':sources,\n",
    "    'confidence':confidence\n",
    "  }\n",
    "  if return_context:\n",
    "    output['context'] = context\n",
    "  return output\n",
    "\n",
    "#Example usage\n",
    "result = rag_advanced(\"Give complete list of the core skills required t o build agentic AI?\", rag_retriever, llm, top_k=5, min_score=0.1, return_context = True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StandardRagDemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
